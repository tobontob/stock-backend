#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„ í’ˆì§ˆ ì§€í‘œ ì¶”ì 
"""

import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
import json
import os

logger = logging.getLogger(__name__)

@dataclass
class AnalysisMetrics:
    """ë¶„ì„ í’ˆì§ˆ ì§€í‘œ"""
    sentiment_confidence: float
    keyword_diversity: int
    stock_extraction_confidence: float
    explanation_quality: int
    processing_time: float
    text_length: int
    stock_count: int
    keyword_count: int

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ"""
    total_processed: int
    total_failed: int
    avg_processing_time: float
    success_rate: float
    start_time: datetime
    end_time: datetime

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.metrics_history: List[AnalysisMetrics] = []
        self.performance_history: List[PerformanceMetrics] = []
        self.current_batch_start = None
        self.current_batch_metrics = []
    
    def start_batch_monitoring(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.current_batch_start = time.time()
        self.current_batch_metrics = []
        logger.info("ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def record_analysis_metrics(self, metrics: AnalysisMetrics):
        """ê°œë³„ ë¶„ì„ í’ˆì§ˆ ì§€í‘œ ê¸°ë¡"""
        self.current_batch_metrics.append(metrics)
        self.metrics_history.append(metrics)
        
        # ì‹¤ì‹œê°„ í’ˆì§ˆ ì²´í¬
        self._check_quality_thresholds(metrics)
    
    def _check_quality_thresholds(self, metrics: AnalysisMetrics):
        """í’ˆì§ˆ ì„ê³„ê°’ ì²´í¬"""
        warnings = []
        
        if metrics.sentiment_confidence < 0.5:
            warnings.append(f"ë‚®ì€ ê°ì •ë¶„ì„ ì‹ ë¢°ë„: {metrics.sentiment_confidence}")
        
        if metrics.stock_extraction_confidence < 0.6:
            warnings.append(f"ë‚®ì€ ì¢…ëª©ì¶”ì¶œ ì‹ ë¢°ë„: {metrics.stock_extraction_confidence}")
        
        if metrics.keyword_diversity < 3:
            warnings.append(f"ë‚®ì€ í‚¤ì›Œë“œ ë‹¤ì–‘ì„±: {metrics.keyword_diversity}")
        
        if metrics.processing_time > 10.0:
            warnings.append(f"ê¸´ ì²˜ë¦¬ ì‹œê°„: {metrics.processing_time:.2f}ì´ˆ")
        
        if warnings:
            logger.warning(f"í’ˆì§ˆ ê²½ê³ : {'; '.join(warnings)}")
    
    def end_batch_monitoring(self, total_processed: int, total_failed: int):
        """ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        if not self.current_batch_start:
            return
        
        end_time = time.time()
        processing_time = end_time - self.current_batch_start
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        success_rate = total_processed / (total_processed + total_failed) if (total_processed + total_failed) > 0 else 0
        avg_processing_time = sum(m.processing_time for m in self.current_batch_metrics) / len(self.current_batch_metrics) if self.current_batch_metrics else 0
        
        performance_metrics = PerformanceMetrics(
            total_processed=total_processed,
            total_failed=total_failed,
            avg_processing_time=avg_processing_time,
            success_rate=success_rate,
            start_time=datetime.fromtimestamp(self.current_batch_start),
            end_time=datetime.fromtimestamp(end_time)
        )
        
        self.performance_history.append(performance_metrics)
        
        # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥
        self._print_performance_report(performance_metrics)
        
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {total_processed}ê°œ, ì‹¤íŒ¨ {total_failed}ê°œ, í‰ê·  ì²˜ë¦¬ì‹œê°„ {avg_processing_time:.2f}ì´ˆ")
    
    def _print_performance_report(self, metrics: PerformanceMetrics):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸")
        print("="*60)
        print(f"ì²˜ë¦¬ëœ ë‰´ìŠ¤: {metrics.total_processed}ê°œ")
        print(f"ì‹¤íŒ¨í•œ ë‰´ìŠ¤: {metrics.total_failed}ê°œ")
        print(f"ì„±ê³µë¥ : {metrics.success_rate:.1%}")
        print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {metrics.avg_processing_time:.2f}ì´ˆ")
        print(f"ì´ ì†Œìš”ì‹œê°„: {(metrics.end_time - metrics.start_time).total_seconds():.2f}ì´ˆ")
        print("="*60)
    
    def get_quality_summary(self, days: int = 7) -> Dict[str, Any]:
        """í’ˆì§ˆ ìš”ì•½ í†µê³„"""
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_metrics = [m for m in self.metrics_history if hasattr(m, 'timestamp') and m.timestamp > cutoff_date]
        
        if not recent_metrics:
            return {}
        
        return {
            "avg_sentiment_confidence": sum(m.sentiment_confidence for m in recent_metrics) / len(recent_metrics),
            "avg_stock_confidence": sum(m.stock_extraction_confidence for m in recent_metrics) / len(recent_metrics),
            "avg_keyword_diversity": sum(m.keyword_diversity for m in recent_metrics) / len(recent_metrics),
            "avg_processing_time": sum(m.processing_time for m in recent_metrics) / len(recent_metrics),
            "total_analyses": len(recent_metrics)
        }
    
    def export_metrics(self, filename: str = None):
        """ì§€í‘œ ë‚´ë³´ë‚´ê¸°"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"analysis_metrics_{timestamp}.json"
        
        export_data = {
            "metrics_history": [asdict(m) for m in self.metrics_history],
            "performance_history": [asdict(m) for m in self.performance_history],
            "export_timestamp": datetime.now().isoformat()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ì§€í‘œê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename

class QualityAnalyzer:
    """ë¶„ì„ í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.quality_thresholds = {
            "sentiment_confidence": 0.6,
            "stock_confidence": 0.7,
            "keyword_diversity": 3,
            "processing_time": 5.0
        }
    
    def analyze_quality(self, metrics: AnalysisMetrics) -> Dict[str, Any]:
        """ê°œë³„ ë¶„ì„ í’ˆì§ˆ í‰ê°€"""
        quality_score = 0
        issues = []
        
        # ê°ì •ë¶„ì„ ì‹ ë¢°ë„ í‰ê°€
        if metrics.sentiment_confidence >= self.quality_thresholds["sentiment_confidence"]:
            quality_score += 25
        else:
            issues.append("ë‚®ì€ ê°ì •ë¶„ì„ ì‹ ë¢°ë„")
        
        # ì¢…ëª©ì¶”ì¶œ ì‹ ë¢°ë„ í‰ê°€
        if metrics.stock_extraction_confidence >= self.quality_thresholds["stock_confidence"]:
            quality_score += 25
        else:
            issues.append("ë‚®ì€ ì¢…ëª©ì¶”ì¶œ ì‹ ë¢°ë„")
        
        # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± í‰ê°€
        if metrics.keyword_diversity >= self.quality_thresholds["keyword_diversity"]:
            quality_score += 25
        else:
            issues.append("ë‚®ì€ í‚¤ì›Œë“œ ë‹¤ì–‘ì„±")
        
        # ì²˜ë¦¬ì‹œê°„ í‰ê°€
        if metrics.processing_time <= self.quality_thresholds["processing_time"]:
            quality_score += 25
        else:
            issues.append("ê¸´ ì²˜ë¦¬ì‹œê°„")
        
        return {
            "quality_score": quality_score,
            "grade": self._get_quality_grade(quality_score),
            "issues": issues,
            "recommendations": self._get_recommendations(issues)
        }
    
    def _get_quality_grade(self, score: int) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if score >= 90:
            return "A+"
        elif score >= 80:
            return "A"
        elif score >= 70:
            return "B+"
        elif score >= 60:
            return "B"
        elif score >= 50:
            return "C"
        else:
            return "D"
    
    def _get_recommendations(self, issues: List[str]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        if "ë‚®ì€ ê°ì •ë¶„ì„ ì‹ ë¢°ë„" in issues:
            recommendations.append("ê°ì •ë¶„ì„ ëª¨ë¸ì˜ ì •í™•ë„ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        if "ë‚®ì€ ì¢…ëª©ì¶”ì¶œ ì‹ ë¢°ë„" in issues:
            recommendations.append("ì¢…ëª©ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ë„ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        if "ë‚®ì€ í‚¤ì›Œë“œ ë‹¤ì–‘ì„±" in issues:
            recommendations.append("í‚¤ì›Œë“œ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¤ì–‘ì„± í™•ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        if "ê¸´ ì²˜ë¦¬ì‹œê°„" in issues:
            recommendations.append("ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        return recommendations

# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
performance_monitor = PerformanceMonitor()
quality_analyzer = QualityAnalyzer()

def monitor_analysis_performance(func):
    """ë¶„ì„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            
            # ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
            processing_time = time.time() - start_time
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶”ì • (ì²« ë²ˆì§¸ ì¸ìê°€ í…ìŠ¤íŠ¸ë¼ê³  ê°€ì •)
            text_length = len(args[0]) if args else 0
            
            # ê²°ê³¼ì—ì„œ ì§€í‘œ ì¶”ì¶œ
            sentiment_confidence = result.get('sentiment', {}).get('score', 0) if isinstance(result, dict) else 0
            stock_count = len(result.get('related_stocks', [])) if isinstance(result, dict) else 0
            keyword_count = len(result.get('financial_keywords', {}).get('stock_keywords', [])) if isinstance(result, dict) else 0
            
            metrics = AnalysisMetrics(
                sentiment_confidence=sentiment_confidence,
                keyword_diversity=keyword_count,
                stock_extraction_confidence=0.7,  # ê¸°ë³¸ê°’
                explanation_quality=len(result.get('reason', '')) if isinstance(result, dict) else 0,
                processing_time=processing_time,
                text_length=text_length,
                stock_count=stock_count,
                keyword_count=keyword_count
            )
            
            performance_monitor.record_analysis_metrics(metrics)
            
            return result
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    return wrapper 